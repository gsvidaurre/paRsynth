---
title: "Vignette_03_Analysis"
author: "GAJ"
date: "2024-12-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<h2>How to Analyze paRsynth Outputs</h2>

In this vignette, we will go through how to analyze the synthetic vocalization generated by the paRsynth package for downstream bioacoustics analysis. 

<u> Step 1: Create a warbleR selection table </u>

To organize all the paRsynth generated outputs you created in the previous vignettes, you will have to create a warbleR selection table. This package is very useful to streamline bioacustic analysis. First you will list all the .wav files then organize them in a selection table.

```{r}

# Create a vector of all of the audio files in the analysis path
wavs <- list.files(path = audio_path, pattern = ".wav$", full.names = FALSE)
length(wavs)
head(wavs)

# w <- 1 # testing

sel_tbl <- data.table::rbindlist(lapply(1:length(wavs), function(w){
  
  tmp <- tuneR::readWave(file.path(audio_path, wavs[w]))
  
  # Return the metadata for the given call using the synthetic metadata generated during audio file generation above
  
  metadats_tmp <- synthetic_call_metadata %>%
    dplyr::filter(grepl(wavs[w], audio_file_name))
  
  # glimpse(metadats_tmp)
  
  # Create a row for the selection table in warbleR format if the audio file exists and metadata for that file also exists
  if(nrow(metadats_tmp) > 0){
    
    # Use 0.1s as a margin to indicate where the vocalization starts and ends in the audio file
    # soundgen::soundgen() adds 100ms of silence before and after the synthetic vocalization by default 
    res <- data.frame(
      sound.files = wavs[w], 
      selec = 1,
      start = 0.1, 
      end = seewave::duration(tmp) - 0.1,
      sampling_rate = tmp@samp.rate,
      group_ID = metadats_tmp[["Group"]],
      individual_ID = metadats_tmp[["Individual"]],
      call_ID = metadats_tmp[["Call"]]
    )
    
  } else {
    
    res <- data.frame(
      sound.files = wavs[w], 
      selec = 1, 
      start = 0.1, 
      end = seewave::duration(tmp) + 0.1,
      sampling_rate = NA,
      group_ID = NA,
      individual_ID = NA,
      call_ID = NA
    )
    
  }
  
  return(res)
  
}))

# See the selection table in warbleR format with all useful metadata for the synthetic experiment
glimpse(sel_tbl)

unique(sel_tbl$sampling_rate)

```

This will output a neatly organized selection table of the unique generated vocalizations.

<u> Step 2: Create spectrogram image files of each audio file </u>

Using the warbleR package, spectrogram images can be created from each synthetic vocalization you generated with paRsynth. Using a spectrogram is beneficial to analyze vocalizations because it provides precise information on frequency over time. First, you will use warbleR to create spectrograms of the generated vocalization in the selection table. Then you will rename the spectrograms image files save in your images_path.

```{r}

warbleR::spectrograms(sel_tbl, wl = 512, flim = c(0, 12), wn = "hanning", pal = reverse.gray.colors.2,ovlp = 90, inner.mar = c(5, 4, 4, 2), outer.mar = c(0, 0, 0, 0), picsize = 1, res = 100, cexlab = 1, propwidth = FALSE, xl = 1, osci = FALSE, gr = FALSE, sc = FALSE, line = FALSE, mar = 0.05, it = "jpeg", parallel = 1, path = audio_path, pb = TRUE, fast.spec = FALSE, by.song = NULL, sel.labels = NULL, title.labels = NULL, dest.path = images_path, box = TRUE, axis = TRUE)

# Rename the image files
imgs <- list.files(images_path)
imgs

new_nms <- gsub(".wav-1", "", imgs)
new_nms

invisible(file.rename(file.path(images_path, imgs), file.path(images_path, new_nms)))

```

This will output appropriately named spectrograms for the generated vocalizations in the selection table created previously. 

<u> Step 3: Perform spectrogrphic cross-correlation (SPCC) to measure acoustic similarity of all calls in dataset (pairwise similarity matrix) </u>

Now, we are getting the key analysis step. Using warbleR, an spectrographic cross-correlation (SPCC) can be performed to find acoustic similarities in the spectrograms created. 

```{r}

glimpse(sel_tbl)

xc_mat <- warbleR::cross_correlation(sel_tbl, wl = 512, ovlp = 90, bp = c(0, 20), wn = "hanning", cor.method = "pearson", parallel = 1, na.rm = FALSE, type = "spectrogram", path = audio_path)

str(xc_mat)
dim(xc_mat)

# xc_mat[1:5, 16:24]

xc_mat[xc_mat < 0]

# Get the percentage of values that are negative
neg_values <- length(xc_mat[xc_mat<0])/(dim(xc_mat)[1]*dim(xc_mat)[2])

# Stop the function if the percentage of negative values is > 0%
if(neg_values > 0){
  stop("Negative values are present in the spectrographic cross-correlation matrix")
}

xc_mat[xc_mat < 0]

# Update the dimension names of the matrix
dimnames(xc_mat) <- list(sel_tbl$sound.files, sel_tbl$sound.files)

```

This will output data about the measured acoustic similarities of all the vocalizations in the dataset using the previously created spectrograms using warbleR.

<u> Step 4: Create acoustic space plots for the vocalizations </u>

Using the results from the SPCC, acoustic space plots can be used as a figure to represent the acoustic similarity of all the calls in the dataset. 

```{r}

# View(xc_mat)

# Convert to a distance matrix and dist object for isoMDS
dist_mat <- stats::as.dist(1 - xc_mat, diag = TRUE, upper = TRUE)
# str(dist_mat)

iso <- invisible(MASS::isoMDS(dist_mat, k = 2, maxit = 1000, trace = FALSE))
str(iso)

mds_df <- data.frame(
  sound.files = dimnames(xc_mat)[[1]],
  X = as.vector(iso$points[, 1]), 
  Y = as.vector(iso$points[, 2]),
) %>% 
  inner_join(
    sel_tbl %>% 
      dplyr::select(sound.files, group_ID, individual_ID),
    by = c("sound.files")
  ) %>% 
  dplyr::mutate(
    group_ID = paste("group", group_ID, sep = "_"),
    individual_ID = paste("individual", individual_ID, sep = "_"),
    unique_individuals = paste(group_ID, individual_ID, sep = " - "),
    group_ID = factor(group_ID),
    individual_ID = factor(individual_ID),
    unique_individuals = factor(unique_individuals)
  )

glimpse(mds_df)
# View(mds_df)

levels(mds_df$group_ID)
levels(mds_df$individual_ID)
levels(mds_df$unique_individuals)

# Colors by group
cols <- scales::alpha(c("navy", "orange", "forestgreen", "firebrick", "grey"), 0.65)

# Shapes by individual within groups
shps <- rep(c(21, 23, 24, 6), length(levels(mds_df$group_ID)))

# Convex hull polygons per group or unique individual
# hulls <- plyr::ddply(mds_df, "unique_individuals", function(x){
#   x[chull(x$X, x$Y), ]
# })

hulls <- plyr::ddply(mds_df, "group_ID", function(x){
  x[chull(x$X, x$Y), ]
})

glimpse(hulls)

mds_df %>%
  ggplot(aes(x = X, y = Y, color = group_ID)) + 
  geom_polygon(data = hulls, aes(x = X, y = Y, fill = group_ID, color = group_ID), alpha = 0.2, size = 0.2, show.legend = FALSE) +
  geom_point(aes(fill = group_ID, color = group_ID, shape = individual_ID), size = 2, stroke = 0.5) +
  scale_shape_manual(values = shps) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = cols) +
  xlab("") + ylab("") + 
  guides(color = "none", shape = "none", fill = "none") +
  theme_bw() +
  theme(
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 9),
    strip.text.x = element_text(size = 12),
    strip.text.y = element_text(size = 10),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.ticks = element_line(size = 0.25),
    plot.margin = margin(0, 3, -3, -3)
  )

```

This will output acoustic space plots to represent the acoustic similarity of the paRsynth generated vocalizations using the spectrograms created previously. 

Run the same steps above, but instead create a data set of synthetic vocalizations with more individual information than group information (This will require changes to the arguments in generate_strings(), as well as the prefix used in write_audio().)

Compare what your did with our results with individual information = TKTK and group information = TKTK. 

TKTK Placeholder TKTK
<br>
![paRsynth graphic workflow](~/Desktop/BIRDS/WorkflowGraphics/paRsynthMethods/paRsynth_methods_workflow.svg)
