---
title: "Vignette 03: Bioacoustics Analysis"
author: "GAJ"
date: "2024-12-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this vignette, we will go through how to analyze the synthetic identity signals you generated using the paRsynth package in vignette 02 by creating spectrograms and acoustic plots to analyze their acoustic similarities.

<h1>Set Up Working Environment</h1>

Before using the functions in the paRsynth package, there are some preliminary steps we must go through to create a directory where different types of generated data can be automatically stored (audios and images). This automatic organization of generated data will make it easier to streamline the process of using paRsynth's functions.

**Re-load packages and reset path**

It is good practice to clean your global environment before using functions from open-sourced packages. In order for paRsynth functions to run, they need specific external R packages as well. For example, 'write_audio' generates and saves audio files in .wav format which requires the package `soundgen::soundgen()` to be installed and loaded. 

```{r message = FALSE, warning = FALSE}

# Clean the global environment
rm(list = ls())

# Specify the required packages
X <- c("devtools", "dplyr", "stringdist", "tidyverse", "ggplot2", "apcluster", "soundgen", "parallel", "stringr", "data.table", "tuneR", "pbapply", "warbleR", "magrittr", "MASS")

# Install the packages in X if not already installed
is_installed <- function(p) is.element(p, installed.packages()[,1])

invisible(lapply(1:length(X), function(x){
  if(!is_installed(X[x])){
    install.packages(X[x], repos = "http://lib.stat.cmu.edu/R/CRAN")
  }
}))

# Install the paRsynth package from GitHub if you haven't installed it already
# devtools::install_github("gsvidaurre/paRsynth")

# Add paRsynth to the list of packages to load
# X <- c(X, "paRsynth")

# Change "Desktop/.../GitHub_repos" based on where paRsynth is stored on your local machine
# testing_path <- "/Users/summe/OneDrive/Desktop/github_repo/paRsynth/R/"
testing_path <- "~/Desktop/BIRDS/GitHub_repos/paRsynth/R/" # Alexandra's testing path

# Load all of the packages specified above
invisible(lapply(X, library, character.only = TRUE))

```

**Re-initialize working directories for data on your local machine**

It is important to have your files organized, especially when creating many different types of output data (audios and images). Therefore, initializing working directories in the beginning will be helpful in understanding the data you create with paRsynth and where they are stored.

```{r}

# Initialize a base path (this will need to be different per user)
# path <- "/Users/raneemsamman/Desktop" # Raneem's path
# path <- "/Users/gracesmith-vidaurre/Desktop" # Grace's path
path <- "/Users/gretheljuarez/Desktop/BIRDS/" # Alexandra's path
# path <- "/Users/summe/OneDrive/Desktop/"

# Initialize the directory for analysis on your local computer (this is likely the name of the folder all of you data is being stored in)
analysis_dir <- "paRsynth_methods_synthetic_dataset"

# Combine the base path and the data directory into a single path
analysis_path <- file.path(path, analysis_dir)

# Create the data directory if it doesn't already exist on your computer
if(!dir.exists(analysis_path)){ 
  dir.create(analysis_path)
}

# Specify a folder inside the analysis directory where audio will be written out/read in
audio_dir <- "audio"

# Combine the base path, the analysis directory, and the audio directory into a single path
audio_path <- file.path(path, analysis_dir, audio_dir)

# Create the audio directory if it doesn't already exist on your computer
if(!dir.exists(audio_path)){ 
  dir.create(audio_path)
}

# Specify a folder inside the analysis directory where images will be written out/read in
images_dir <- "images"

# Combine the base path, the analysis directory, and the data directory into a single path
images_path <- file.path(path, analysis_dir, images_dir)

# Create the data directory if it doesn't already exist on your computer
if(!dir.exists(images_path)){ 
  dir.create(images_path)
}

# Specify a folder inside the analysis directory where images for figures will be written out/read in
figures_dir <- "figures"

# Combine the base path, the analysis directory, and the data directory into a single path
figures_path <- file.path(path, analysis_dir, figures_dir)

# Create the data directory if it doesn't already exist on your computer
if(!dir.exists(figures_path)){ 
  dir.create(figures_path)
}

```

**Read in the synthetic call metadata and set cores for parallel processing**

```{r}

synthetic_call_metadata <- read.csv(file.path(analysis_path, "synthetic_call_metadata.csv"))
glimpse(synthetic_call_metadata)

# Remove NAs
synthetic_call_metadata_clean <- na.omit(synthetic_call_metadata)

cores <- parallel::detectCores() - 4
cores

# Find the maximum frequency anchor value across all vocalizations
freq_cols <- synthetic_call_metadata_clean[, grepl("Frequency", names(synthetic_call_metadata))]
glimpse(freq_cols)

max_freq <- max(sapply(freq_cols, max))
max_freq

```

<h1>Bioacoustics Analysis Workflow</h1>

**Step 1: Create a warbleR selection table**

To organize all the paRsynth generated outputs you created in the previous vignettes, you will have to create a warbleR selection table. The warbleR package is very useful to streamline bioacustics analysis. First you will list all the .wav files and then organize them in a selection table.

```{r}

# Create a vector of all of the audio files in the analysis path
wavs <- list.files(path = audio_path, pattern = ".wav$", full.names = FALSE)
length(wavs)
head(wavs)
tail(wavs)

# Iterate over the audio files to create one row of the selection table at a time. warbleR selection tables have a very specific format that must be used for downstream analysis.
sel_tbl <- data.table::rbindlist(pblapply(1:length(wavs), function(w){
  
  tmp <- tuneR::readWave(file.path(audio_path, wavs[w]))
  
  # Return the metadata for the given call using the synthetic metadata generated during audio file generation above
  metadats_tmp <- synthetic_call_metadata %>%
    dplyr::filter(grepl(wavs[w], audio_file_name))
  
  # Create a row for the selection table in warbleR format if the audio file exists and metadata for that file also exists
  if(nrow(metadats_tmp) > 0){
    
    # Use 0.1s as a margin to indicate where the vocalization starts and ends in the audio file
    # soundgen::soundgen() adds 100ms of silence before and after the synthetic vocalization by default 
    res <- data.frame(
      sound.files = wavs[w], 
      selec = 1,
      start = 0.1, 
      end = seewave::duration(tmp) - 0.1,
      sampling_rate = tmp@samp.rate,
      group_ID = metadats_tmp[["Group"]],
      individual_ID = metadats_tmp[["Individual"]],
      call = metadats_tmp[["Call"]],
      call_ID = metadats_tmp[["Call_ID"]],
      dataset = metadats_tmp[["dataset"]]
    )
    
  } else {
    
    res <- data.frame(
      sound.files = wavs[w], 
      selec = 1, 
      start = 0.1, 
      end = seewave::duration(tmp) + 0.1,
      sampling_rate = NA,
      group_ID = NA,
      individual_ID = NA,
      call = NA,
      call_ID = NA,
      dataset = NA
    )
    
  }
  
  return(res)
  
}))

# See the selection table in warbleR format with useful metadata for acoustic space plots
glimpse(sel_tbl)

# Check the selection table for all the unique sampling rates
unique(sel_tbl$sampling_rate)

```

This will output a neatly organized selection table of the uniquely generated vocalizations and their corresponding metadata.

**Step 2: Create spectrogram image files of each audio file**

Using the warbleR package, spectrogram images can be created from each synthetic identity signals you generated with paRsynth. Using a spectrogram is beneficial to analyze vocalizations because it provides precise information on frequency over time. First, you will use warbleR to create spectrograms of the generated vocalization in the selection table. Then you will rename the spectrogram image files saved in your images_path. 

In the following example, we are creating spectrograms from the first dataset we created, where more group information is encoded in the vocalization than individual identity information.

```{r}

# To make spectrograms of dataset 1 (more group info), you will need to filter it out from the selection table you made above.
GroupMembership_sel_tbl <- sel_tbl %>%
    dplyr::filter(grepl("GroupMembership", sound.files))

# Create spectrograms for filtered calls
warbleR::spectrograms(GroupMembership_sel_tbl, wl = 512, flim = c(0, 12), wn = "hanning", pal = reverse.gray.colors.2,ovlp = 90, inner.mar = c(5, 4, 4, 2), outer.mar = c(0, 0, 0, 0), picsize = 1, res = 100, cexlab = 1, propwidth = FALSE, xl = 1, osci = FALSE, gr = FALSE, sc = FALSE, line = FALSE, mar = 0.05, it = "jpeg", parallel = 1, path = audio_path, pb = TRUE, fast.spec = FALSE, by.song = NULL, sel.labels = NULL, title.labels = NULL, dest.path = images_path, box = TRUE, axis = TRUE)

# Rename the image files
imgs <- list.files(images_path)

new_nms <- gsub(".wav-1", "", imgs)

invisible(file.rename(file.path(images_path, imgs), file.path(images_path, new_nms)))

```

This will output appropriately named spectrograms for the synthetically generated vocalizations in the selection table created previously. Refer to the image below for an example of a spectrogram (GroupMembership_Group1_Ind1_Call1).

![Example Spectrogram of GroupMembership_Group1_Ind1_Call1](./images/GroupMembership_Group1_Indiv1_Call1_vignettes.png)

<br>

**Step 3: Perform spectrogrphic cross-correlation (SPCC) to measure acoustic similarity of all calls in dataset (pairwise similarity matrix)**

Now, we are getting to the key analysis step. Using warbleR, a spectrographic cross-correlation (SPCC) can be performed to find acoustic similarities between the spectrograms created. The output is single similarity measurement for every unique vocalization compared to the dataset. 

If the `temperature` argument used by `soundgen` is set to 0 (for no stochasticity in sound generation), then you will need to filter out any calls that have duplicated strings prior to running cross-correlation. Otherwise you will have values of 1 outside of the diagonal representing comparisons among calls of exactly the same structure or duplicates, and multidimensional scaling will fail.


```{r}

# Perform SPCC to measure acoustic similarity of all calls in dataset GroupMembership_sel_tbl
# When generating the synthetic vocalizations, increasing the temperature from 0 to 0.025 reduced the number of ones off of the diagonal to 1. I just increased temperature to 0.05 and there was still a single 1 off of the diagonal
# (max_freq/1000): The maximum frequency in the bandwidth argument was 7 kHz, add 2 kHz for a buffer
xc_mat <- warbleR::cross_correlation(GroupMembership_sel_tbl, wl = 512, ovlp = 90, bp = c(0, 20), wn = "hanning", cor.method = "pearson", parallel = 1, na.rm = FALSE, type = "spectrogram", path = audio_path)

# Check the structure of xc_mat
str(xc_mat)

# Check the dimensions of xc_mat
dim(xc_mat)

# Get the percentage of values that are negative
        neg_values <- ((length(xc_mat[xc_mat < 0])/length(xc_mat))) * 100

# Set all negative values to the minimum positive value in the SPCC matrix to represent the fact that these calls are very structurally different from one another
        if(neg_values > 0){
          
          xc_mat[xc_mat < 0] <- round(min(xc_mat[xc_mat > 0]), 8)
          
        }
        
          # Continue with downstream analyses only if there are no 1's off of the diagonal, otherwise MDS will fail. Record NA values if there are 1's off of the diagonal (e.g. two calls of the same structure will present values of 1 as if the same call were compared to itself)
        
        # Make sure to calculate the true number of values in the symmetric matrix (multiply the number of unique values that are 1 by 2)
        ones <- ((length(which(xc_mat[lower.tri(xc_mat)] == 1))*2)/length(xc_mat)) * 100
        
        if(ones > 0){
          
          # Get the maximum similarity value after 1 in the matrix
          max_vals <- xc_mat[lower.tri(xc_mat)]
          
          # Use a value just under one to catch floating point numbers that are visually represented as 1
          sec_max_val <- max(max_vals[max_vals < 0.9999999])
          
          # Then set the values of 1 off of the diagonal (in both the upper and lower triangle) to the next highest similarity value in this matrix
          xc_mat[lower.tri(xc_mat)][xc_mat[lower.tri(xc_mat)] == 1] <- round(sec_max_val, 8)
          xc_mat[upper.tri(xc_mat)][xc_mat[upper.tri(xc_mat)] == 1] <- round(sec_max_val, 8)
          
        }

# Update the dimension names of the matrix to include the sound file names from the selection table to keep track of the vocalizations
dimnames(xc_mat) <- list(GroupMembership_sel_tbl$sound.files, GroupMembership_sel_tbl$sound.files)

# Save this matrix since it can take a long time to generate when there are many vocalizations
saveRDS(xc_mat, file.path(analysis_path, "xc_mat.RDS"))

```

This will output data about the measured acoustic similarities of all the vocalizations in the dataset using the previously created spectrograms.

**Step 4: Create acoustic space plots for the vocalizations**

Using the results from the SPCC, acoustic space plots can be used as a figure to represent the acoustic similarity of all the calls in the dataset. We can then apply an unsupervised machine learning approach, multidemnsional scaling (mds). Here we will use the vocalizations where more group information is encoded within the calls.

```{r}

# Convert to a distance matrix and dist object for isoMDS (helps visualize the data in a simplified way)
dist_mat <- stats::as.dist(1 - xc_mat, diag = TRUE, upper = TRUE)

# Check the structure of dist_mat
str(dist_mat)

# Perform isoMDS to reduce the dimensionality of the data to 2 dimensions
iso <- invisible(MASS::isoMDS(dist_mat, k = 2, maxit = 1000, trace = FALSE))

# Check the structure of iso
str(iso)

# Create a data frame with the MDS coordinates and the group and individual IDs
mds_df <- data.frame(
  sound.files = dimnames(xc_mat)[[1]],
  X = as.vector(iso$points[, 1]),
  Y = as.vector(iso$points[, 2])
) %>% 
  inner_join( # inner join the data frame with the selection table to include group and individual IDs
    GroupMembership_sel_tbl %>% 
      dplyr::select(sound.files, group_ID, individual_ID),
    by = c("sound.files")
  ) %>% 
  # glimpse()
  dplyr::mutate(
    unique_individuals = paste(group_ID, individual_ID, sep = " - "), # create a new column with the group and individual IDs separated by a hyphen
    group_ID = factor(group_ID), # convert the group ID to a factor
    individual_ID = factor(individual_ID), # convert the individual ID to a factor
    unique_individuals = factor(unique_individuals) # convert the unique individual ID to a factor
  )

# 'levels' provides the unique values of a factor. This is useful for checking the unique values of the group and individual IDs.
levels(mds_df$group_ID) 
levels(mds_df$individual_ID)
levels(mds_df$unique_individuals)

# Colors by group
cols <- scales::alpha(c("navy", "orange"), 0.65)

# Shapes by individual within groups
shps <- c(0, 1, 2, 5, 6, 15, 19, 17, 18, 14)

# Create a data frame which has calculation of the convex hull for each group (aka the smallest polygon that can enclose all the points in a group). This is useful for visualizing the spread of the data in the acoustic space plot (like the data istribution, boundaries in a scatter plot)
hulls <- plyr::ddply(mds_df, "group_ID", function(x){
  x[chull(x$X, x$Y), ]
})

# Piping the data frame into ggplot to create the acoustic space plot
GroupMembership <- mds_df %>%
  ggplot(aes(x = X, y = Y, color = group_ID)) + 
  geom_polygon(data = hulls, aes(x = X, y = Y, fill = group_ID, color = group_ID), alpha = 0.2, size = 0.2, show.legend = FALSE) +
  geom_point(aes(fill = group_ID, color = group_ID, shape = individual_ID), size = 2, stroke = 0.5) +
  scale_shape_manual(values = shps) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = cols) +
  guides(shape = guide_legend(nrow = 1, title = "Individual"), color = guide_legend(nrow = 1, title = "Group"), fill = guide_legend(nrow = 1, title = "Group")) +
  xlab("MDS Dimension 1") + ylab("MDS Dimension 2") +
  theme_bw() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 8),
    strip.text.x = element_text(size = 12),
    strip.text.y = element_text(size = 10),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.ticks = element_line(size = 0.25),
    plot.margin = margin(10, 10, 10, 10),
    legend.position = "top",
    legend.key.spacing = unit(0.1, "lines"),
    legend.box.spacing = unit(0.1, "lines")
  )

ggsave("GroupMembership.png", plot = GroupMembership, path = figures_path)

```

![Example Acoustic Space Plot: Group Membership ](./images/GroupMembership_vignettes.png)

<br>

This will output an acoustic space plot to represent the acoustic similarity of the paRsynth generated vocalizations using the spectrograms created in vignette 02. In this case, we generated space plots for the first dataset that has more group information encoded in the vocalizations. It will not look the same as the plot you generated because paRsynth randomly generates vocalizations strings, but the overall trends should be similar


<h1>2nd Example: Individual Information > Group Information</h1>

Now lets run this code to analyze the acoustic similarities for the 2nd example dataset where more individual information is encoded within the calls than group information.

```{r}

IndividualIdentity_sel_tbl <- sel_tbl %>%
    dplyr::filter(grepl("IndividualIdentity", sound.files))
```

```{r eval = FALSE}
# Create spectrograms for the calls in IndividualIdentity_sel_tbl
warbleR::spectrograms(IndividualIdentity_sel_tbl, wl = 512, flim = c(0, 12), wn = "hanning", pal = reverse.gray.colors.2,ovlp = 90, inner.mar = c(5, 4, 4, 2), outer.mar = c(0, 0, 0, 0), picsize = 1, res = 100, cexlab = 1, propwidth = FALSE, xl = 1, osci = FALSE, gr = FALSE, sc = FALSE, line = FALSE, mar = 0.05, it = "jpeg", parallel = 1, path = audio_path, pb = TRUE, fast.spec = FALSE, by.song = NULL, sel.labels = NULL, title.labels = NULL, dest.path = images_path, box = TRUE, axis = TRUE)

# a list of all the image files in the images_path
imgs <- list.files(images_path)

# remove the "-1" from the image file names
new_nms <- gsub(".wav-1", "", imgs)

# rename the image files
invisible(file.rename(file.path(images_path, imgs), file.path(images_path, new_nms)))
```

```{r}

# Perform SPCC to measure acoustic similarity of all calls in dataset IndividualIdentity_sel_tbl
xc_mat2 <- warbleR::cross_correlation(IndividualIdentity_sel_tbl, wl = 512, ovlp = 90, bp = c(0, 20), wn = "hanning", cor.method = "pearson", parallel = 1, na.rm = FALSE, type = "spectrogram", path = audio_path)

# Check the structure of xc_mat
str(xc_mat2)

# Check the dimensions of xc_mat
dim(xc_mat2)

# Get the percentage of values that are negative
        neg_values2 <- ((length(xc_mat2[xc_mat2 < 0])/length(xc_mat2))) * 100

# Set all negative values to the minimum positive value in the SPCC matrix to represent the fact that these calls are very structurally different from one another
        if(neg_values2 > 0){
          
          xc_mat2[xc_mat2 < 0] <- round(min(xc_mat2[xc_mat2 > 0]), 8)
          
        }

# Make sure to calculate the true number of values in the symmetric matrix (multiply the number of unique values that are 1 by 2)
        ones2 <- ((length(which(xc_mat2[lower.tri(xc_mat2)] == 1))*2)/length(xc_mat2)) * 100
        
        if(ones2 > 0){
          
          # Get the maximum similarity value after 1 in the matrix
          max_vals2 <- xc_mat2[lower.tri(xc_mat2)]
          
          # Use a value just under one to catch floating point numbers that are visually represented as 1
          sec_max_val2 <- max(max_vals2[max_vals2 < 0.9999999])
          
          # Then set the values of 1 off of the diagonal (in both the upper and lower triangle) to the next highest similarity value in this matrix
          xc_mat2[lower.tri(xc_mat2)][xc_mat2[lower.tri(xc_mat2)] == 1] <- round(sec_max_val2, 8)
          xc_mat2[upper.tri(xc_mat2)][xc_mat2[upper.tri(xc_mat2)] == 1] <- round(sec_max_val2, 8)
          
        }

# Update the dimension names of the matrix to include the sound file names from the selection table to keep track of the vocalizations
dimnames(xc_mat2) <- list(IndividualIdentity_sel_tbl$sound.files, IndividualIdentity_sel_tbl$sound.files)

# Save this matrix since it can take a long time to generate when there are many vocalizations
saveRDS(xc_mat, file.path(analysis_path, "xc_mat2.RDS"))

```

```{r}

# Convert to a distance matrix and dist object for isoMDS (helps visualize the data in a simplified way)
dist_mat2 <- stats::as.dist(1 - xc_mat2, diag = TRUE, upper = TRUE)

# Perform isoMDS to reduce the dimensionality of the data to 2 dimensions
iso2 <- invisible(MASS::isoMDS(dist_mat2, k = 2, maxit = 1000, trace = FALSE))
str(iso2)

# Create a data frame with the MDS coordinates and the group and individual IDs
mds_df2 <- data.frame(
  sound.files = dimnames(xc_mat2)[[1]],
  X = as.vector(iso2$points[, 1]), 
  Y = as.vector(iso2$points[, 2])
) %>% 
  # inner join the data frame with the selection table to include group and individual IDs
  inner_join( 
    IndividualIdentity_sel_tbl %>% 
      dplyr::select(sound.files, group_ID, individual_ID),
    by = c("sound.files") # order by the sound.files column
  ) %>% 
  # glimpse()
  dplyr::mutate( 
    unique_individuals = paste(group_ID, individual_ID, sep = " - "), # create a new column with the group and individual IDs separated by a hyphen
    group_ID = factor(group_ID), # convert the group ID to a factor
    individual_ID = factor(individual_ID), # convert the individual ID to a factor
    unique_individuals = factor(unique_individuals) # convert the unique individual ID to a factor
  )

# 'levels' provides the unique values of a factor. This is useful for checking the unique values of the group and individual IDs.
levels(mds_df2$group_ID)
levels(mds_df2$individual_ID)
levels(mds_df2$unique_individuals)

cols <- scales::alpha(c("navy", "orange"), 0.65)

shps <- c(0, 1, 2, 5, 6, 15, 19, 17, 18, 14)

hulls2 <- plyr::ddply(mds_df2, "group_ID", function(x){
  x[chull(x$X, x$Y), ]
})

# piping the data frame into ggplot to create the acoustic space plot
IndividualIdentity <- mds_df2 %>%
  ggplot(aes(x = X, y = Y, color = group_ID)) + 
  geom_polygon(data = hulls2, aes(x = X, y = Y, fill = group_ID, color = group_ID), alpha = 0.2, size = 0.2, show.legend = FALSE) +
  geom_point(aes(fill = group_ID, color = group_ID, shape = individual_ID), size = 2, stroke = 0.5) +
  scale_shape_manual(values = shps) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = cols) +
  xlab("MDS Dimension 1") + ylab("MDS Dimension 2") +
  guides(shape = guide_legend(nrow = 1, title = "Individual"), color = guide_legend(nrow = 1, title = "Group"), fill = guide_legend(nrow = 1, title = "Group")) +
  theme_bw() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 8),
    strip.text.x = element_text(size = 12),
    strip.text.y = element_text(size = 10),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.ticks = element_line(size = 0.25),
    plot.margin = margin(10, 10, 10, 10),
    legend.position = "top",
    legend.key.spacing = unit(0.1, "lines"),
    legend.box.spacing = unit(0.1, "lines")
  )

ggsave("IndividualIdentity.png", plot = IndividualIdentity, path = figures_path)

```

![Example Acoustic Space Plot: Individual Identity ](./images/IndividualIdentity_vignettes.png)

<br>

This will output an acoustic space plot to represent the acoustic similarity of the paRsynth generated vocalizations using the spectrograms created in vignette 02 for the second data set that has more individual information.

This last vignette has showed you how to conduct downstream bioacoustics analysis on two datasets: 1) more group information than individual information and 2) vice versa. You created acoustic space plots for each data sets. You have completed the final vignette of the paRsynth package, well done!:smile:

<h2>Extreme Group-level Variation Example</h2>

You may create vocalization that have much greater group information than individual information to the point when you make the acoustic space plot it result in all the individuals of one group to be clustered on top of each other and each group are a outstandingly great distance from each other. 

![Acoustic Space Plot: Extreme Group-level Variation ](./images/ExtraExample_vignettes.png)

<br>

This is an example of an acoustic plot of a dataset that has too much group information compared to individual information. Two unique dots can be seen at opposite ends of the plot - meaning there is not a lot of relevant information here to conduct acoustic analyses. To solve this, try changing the parameters so that the difference between them are not as striking.
