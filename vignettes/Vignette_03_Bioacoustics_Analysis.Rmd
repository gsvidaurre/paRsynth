---
title: "Vignette 03: Bioacoustics Analysis"
author: "GAJ"
date: "2024-12-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this vignette, we will go through how to analyze the synthetic vocalizations generated by the paRsynth package. By creating spectrograms and acoustic plots, we are able to compare acoustic similarities of the paRsynth generated vocalizations from Vignette 02.

<h1>Set Up Working Environment</h1>

Before analyzing the outputs you generated with paRsynth, there are some preliminary steps we must go through to ensure it will work appropriately and be organized neatly.

**Re-load packages and reset path**

```{r message = FALSE, warning = FALSE}

# Clean the global environment
rm(list = ls())

# Specify the required packages
X <- c("devtools", "dplyr", "stringdist", "tidyverse", "ggplot2", "apcluster", "soundgen", "parallel", "stringr", "data.table", "tuneR", "pbapply", "warbleR", "magrittr")

# Install the packages in X if not already installed
is_installed <- function(p) is.element(p, installed.packages()[,1])

invisible(lapply(1:length(X), function(x){
  if(!is_installed(X[x])){
    install.packages(X[x], repos = "http://lib.stat.cmu.edu/R/CRAN")
  }
}))

# Install the paRsynth package from GitHub if you haven't installed it already
# devtools::install_github("gsvidaurre/paRsynth")

# Add paRsynth to the list of packages to load
# X <- c(X, "paRsynth")

# Change "Desktop/.../GitHub_repos" based on where paRsynth is stored on your local machine
# testing_path <- "/Users/summe/OneDrive/Desktop/github_repo/paRsynth/R/"
testing_path <- "~/Desktop/BIRDS/GitHub_repos/paRsynth/R/"

# Load the paRsynth functions that will be tested below
source(file.path(testing_path, "generate_strings.R"))
source(file.path(testing_path, "parsons_code.R"))
source(file.path(testing_path, "frequency_anchors.R"))
source(file.path(testing_path, "write_audio.R"))

# Load all of the packages specified above
invisible(lapply(X, library, character.only = TRUE))

```

**Re-initialize working directories for data on your local machine**

It is important to have your files organized, especially when creating many different types of output data. Therefore, initializing working directories throughout will be helpful when analyzing the data you create with paRsynth.

```{r eval = TRUE}

# Initialize a base path (this will need to be different per user)
# path <- "/Users/raneemsamman/Desktop" # Raneem's path
# path <- "/Users/gracesmith-vidaurre/Desktop" # Grace's path
path <- "/Users/gretheljuarez/Desktop/BIRDS/" # Alexandra's path
# path <- "/Users/summe/OneDrive/Desktop/"

# Initialize the directory for analysis on your local computer (this is likely the name of the folder all of you data is being stored in)
analysis_dir <- "paRsynth_methods_synthetic_dataset"

# Combine the base path and the data directory into a single path
analysis_path <- file.path(path, analysis_dir)

# Create the data directory if it doesn't already exist on your computer
if(!dir.exists(analysis_path)){ 
  dir.create(analysis_path)
}

# Specify a folder inside the analysis directory where audio will be written out/read in
audio_dir <- "audio"

# Combine the base path, the analysis directory, and the audio directory into a single path
audio_path <- file.path(path, analysis_dir, audio_dir)

# Create the audio directory if it doesn't already exist on your computer
if(!dir.exists(audio_path)){ 
  dir.create(audio_path)
}

# Specify a folder inside the analysis directory where images will be written out/read in
images_dir <- "images"

# Combine the base path, the analysis directory, and the data directory into a single path
images_path <- file.path(path, analysis_dir, images_dir)

# Create the data directory if it doesn't already exist on your computer
if(!dir.exists(images_path)){ 
  dir.create(images_path)
}

```

<h1>Bioacoustics Analysis Steps</h1>

**Step 1: Create a warbleR selection table**

To organize all the paRsynth generated outputs you created in the previous vignettes, you will have to create a warbleR selection table. The warbleR package is very useful to streamline bioacustics analysis. First you will list all the .wav files and then organize them in a selection table.

```{r}

# Create a vector of all of the audio files in the analysis path
wavs <- list.files(path = audio_path, pattern = ".wav$", full.names = FALSE)

# w <- 25 # testing

# Create a selection table including synthetic call metadata and its corresponding audio file name
sel_tbl <- data.table::rbindlist(lapply(1:length(wavs), function(w){
  
  tmp <- tuneR::readWave(file.path(audio_path, wavs[w]))
  
  # Return the metadata for the given call using the synthetic metadata generated during audio file generation above
  
  synthetic_call_metadata <- read.csv("/Users/gretheljuarez/Desktop/BIRDS/paRsynth_methods_synthetic_dataset/synthetic_call_metadata.csv")
  
  metadats_tmp <- synthetic_call_metadata %>%
    dplyr::filter(grepl(wavs[w], audio_file_name))
  
  # glimpse(metadats_tmp)
  
  # Create a row for the selection table in warbleR format if the audio file exists and metadata for that file also exists
  if(nrow(metadats_tmp) > 0){
    
    # Use 0.1s as a margin to indicate where the vocalization starts and ends in the audio file
    # soundgen::soundgen() adds 100ms of silence before and after the synthetic vocalization by default 
    res <- data.frame(
      sound.files = wavs[w], 
      selec = 1,
      start = 0.1, 
      end = seewave::duration(tmp) - 0.1,
      sampling_rate = tmp@samp.rate,
      group_ID = metadats_tmp[["Group"]],
      individual_ID = metadats_tmp[["Individual"]],
      call_ID = metadats_tmp[["Call"]]
    )
    
  } else {
    
    res <- data.frame(
      sound.files = wavs[w], 
      selec = 1, 
      start = 0.1, 
      end = seewave::duration(tmp) + 0.1,
      sampling_rate = NA,
      group_ID = NA,
      individual_ID = NA,
      call_ID = NA
    )
    
  }
  
  return(res)
  
}))

# Check the selection table for all the unique sampling rates
unique(sel_tbl$sampling_rate)

# See the selection table in warbleR format with all useful metadata for the synthetic experiment
glimpse(sel_tbl)

```

This will output a neatly organized selection table of the uniquely generated vocalizations and their corresponding metadata.

**Step 2: Create spectrogram image files of each audio file**

Using the warbleR package, spectrogram images can be created from each synthetic vocalization you generated with paRsynth. Using a spectrogram is beneficial to analyze vocalizations because it provides precise information on frequency over time. First, you will use warbleR to create spectrograms of the generated vocalization in the selection table. Then you will rename the spectrogram image files saved in your images_path. 

In the following example, we are creating spectrograms for the first dataset we created, where more group information is encoded in the vocalization than individual identity information.

```{r}

# To make spectrograms of dataset 1 (more group info), you will need to filter it out from the selection table you made above.
GroupMembership_sel_tbl <- sel_tbl %>%
    dplyr::filter(grepl("GroupMembership", sound.files))

# Create spectrograms for filtered calls
warbleR::spectrograms(GroupMembership_sel_tbl, wl = 512, flim = c(0, 12), wn = "hanning", pal = reverse.gray.colors.2,ovlp = 90, inner.mar = c(5, 4, 4, 2), outer.mar = c(0, 0, 0, 0), picsize = 1, res = 100, cexlab = 1, propwidth = FALSE, xl = 1, osci = FALSE, gr = FALSE, sc = FALSE, line = FALSE, mar = 0.05, it = "jpeg", parallel = 1, path = audio_path, pb = TRUE, fast.spec = FALSE, by.song = NULL, sel.labels = NULL, title.labels = NULL, dest.path = images_path, box = TRUE, axis = TRUE)

# Rename the image files
imgs <- list.files(images_path)

new_nms <- gsub(".wav-1", "", imgs)

invisible(file.rename(file.path(images_path, imgs), file.path(images_path, new_nms)))

```

This will output appropriately named spectrograms for the synthetically generated vocalizations in the selection table created previously. Refer to the image below for an example of a spectrogram (GroupMembership_Group1_Ind1_Call1).

<br>
![Example Spectrogram of GroupMembership_Group1_Ind1_Call1](/Users/gretheljuarez/Desktop/BIRDS/GitHub_repos/paRsynth/vignettes/images/GroupMembership_Group1_Ind1_Call1copy-min.png)

**Step 3: Perform spectrogrphic cross-correlation (SPCC) to measure acoustic similarity of all calls in dataset (pairwise similarity matrix)**

Now, we are getting the key analysis step. Using warbleR, a spectrographic cross-correlation (SPCC) can be performed to find acoustic similarities between the spectrograms created. 

```{r}

xc_mat <- warbleR::cross_correlation(GroupMembership_sel_tbl, wl = 512, ovlp = 90, bp = c(0, 20), wn = "hanning", cor.method = "pearson", parallel = 1, na.rm = FALSE, type = "spectrogram", path = audio_path)

# TKTK What do these functions do?
str(xc_mat)

dim(xc_mat)

# xc_mat[1:5, 16:24] # TKTK Is this for testing?

xc_mat[xc_mat < 0]

# Get the percentage of values that are negative
neg_values <- length(xc_mat[xc_mat<0])/(dim(xc_mat)[1]*dim(xc_mat)[2])

# Stop the function if the percentage of negative values is > 0%
if(neg_values > 0){
  stop("Negative values are present in the spectrographic cross-correlation matrix")
}

xc_mat[xc_mat < 0]

# Update the dimension names of the matrix
dimnames(xc_mat) <- list(GroupMembership_sel_tbl$sound.files, GroupMembership_sel_tbl$sound.files)

```

This will output data about the measured acoustic similarities of all the vocalizations in the dataset using the previously created spectrograms.

**Step 4: Create acoustic space plots for the vocalizations**

Using the results from the SPCC, acoustic space plots can be used as a figure to represent the acoustic similarity of all the calls in the dataset. Here we will use the vocalizations where more group information is encoded within the calls.

```{r}

# Convert to a distance matrix and dist object for isoMDS (helps visualize the data in a simplified way)
dist_mat <- stats::as.dist(1 - xc_mat, diag = TRUE, upper = TRUE)
# str(dist_mat)

iso <- invisible(MASS::isoMDS(dist_mat, k = 2, maxit = 1000, trace = FALSE))
str(iso)

mds_df <- data.frame(
  sound.files = dimnames(xc_mat)[[1]],
  X = as.vector(iso$points[, 1]), 
  Y = as.vector(iso$points[, 2])
) %>% 
  inner_join(
    GroupMembership_sel_tbl %>% 
      dplyr::select(sound.files, group_ID, individual_ID),
    by = c("sound.files")
  ) %>% 
  # glimpse()
  dplyr::mutate(
    unique_individuals = paste(group_ID, individual_ID, sep = " - "),
    group_ID = factor(group_ID),
    individual_ID = factor(individual_ID),
    unique_individuals = factor(unique_individuals)
  )

levels(mds_df$group_ID)
levels(mds_df$individual_ID)
levels(mds_df$unique_individuals)

# Colors by group
cols <- scales::alpha(c("navy", "orange"), 0.65)

# Shapes by individual within groups
shps <- c(0, 1, 2, 5, 6, 15, 19, 17, 18, 14)

# Convex hull polygons per group or unique individual
# hulls <- plyr::ddply(mds_df, "unique_individuals", function(x){
#   x[chull(x$X, x$Y), ]
# })

hulls <- plyr::ddply(mds_df, "group_ID", function(x){
  x[chull(x$X, x$Y), ]
})

mds_df %>%
  ggplot(aes(x = X, y = Y, color = group_ID)) + 
  geom_polygon(data = hulls, aes(x = X, y = Y, fill = group_ID, color = group_ID), alpha = 0.2, size = 0.2, show.legend = FALSE) +
  geom_point(aes(fill = group_ID, color = group_ID, shape = individual_ID), size = 2, stroke = 0.5) +
  scale_shape_manual(values = shps) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = cols) +
  guides(shape = guide_legend(nrow = 1, title = "Individual"), color = guide_legend(nrow = 1, title = "Group"), fill = guide_legend(nrow = 1, title = "Group")) +
  xlab("MDS Dimension 1") + ylab("MDS Dimension 2") +
  theme_bw() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 8),
    strip.text.x = element_text(size = 12),
    strip.text.y = element_text(size = 10),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.ticks = element_line(size = 0.25),
    plot.margin = margin(0, 3, -3, -3),
    legend.position = "top",
    legend.key.spacing = unit(0.1, "lines"),
    legend.box.spacing = unit(0.1, "lines")
  )
  
```

This will output an acoustic space plot to represent the acoustic similarity of the paRsynth generated vocalizations using the spectrograms created in vignette 02. In this case, we generated space plots for the first dataset that has more group information encoded in the vocalizations. It will not look the same as the plot you generated because paRsynth randomly generates vocalizations strings, but the overall trends should be similar


<h1>2nd Example: Individual Information > Group Information</h1>

Now lets run this code to analyze the acoustic similarities for the 2nd example dataset where more individual information is encoded within the calls than group information.

```{r}

IndividualIdentity_sel_tbl <- sel_tbl %>%
    dplyr::filter(grepl("IndividualIdentity", sound.files))
```

```{r eval = FALSE}
warbleR::spectrograms(IndividualIdentity_sel_tbl, wl = 512, flim = c(0, 12), wn = "hanning", pal = reverse.gray.colors.2,ovlp = 90, inner.mar = c(5, 4, 4, 2), outer.mar = c(0, 0, 0, 0), picsize = 1, res = 100, cexlab = 1, propwidth = FALSE, xl = 1, osci = FALSE, gr = FALSE, sc = FALSE, line = FALSE, mar = 0.05, it = "jpeg", parallel = 1, path = audio_path, pb = TRUE, fast.spec = FALSE, by.song = NULL, sel.labels = NULL, title.labels = NULL, dest.path = images_path, box = TRUE, axis = TRUE)

imgs <- list.files(images_path)

new_nms <- gsub(".wav-1", "", imgs)

invisible(file.rename(file.path(images_path, imgs), file.path(images_path, new_nms)))
```

```{r}

xc_mat2 <- warbleR::cross_correlation(IndividualIdentity_sel_tbl, wl = 512, ovlp = 90, bp = c(0, 20), wn = "hanning", cor.method = "pearson", parallel = 1, na.rm = FALSE, type = "spectrogram", path = audio_path)

str(xc_mat2)
dim(xc_mat2)

xc_mat2[xc_mat2 < 0]

neg_values2 <- length(xc_mat2[xc_mat2<0])/(dim(xc_mat2)[1]*dim(xc_mat2)[2])

if(neg_values2 > 0){
  stop("Negative values are present in the spectrographic cross-correlation matrix")
}

xc_mat2[xc_mat2 < 0]

dimnames(xc_mat2) <- list(IndividualIdentity_sel_tbl$sound.files, IndividualIdentity_sel_tbl$sound.files)

dist_mat2 <- stats::as.dist(1 - xc_mat2, diag = TRUE, upper = TRUE)

iso2 <- invisible(MASS::isoMDS(dist_mat2, k = 2, maxit = 1000, trace = FALSE))
str(iso2)

mds_df2 <- data.frame(
  sound.files = dimnames(xc_mat2)[[1]],
  X = as.vector(iso2$points[, 1]), 
  Y = as.vector(iso2$points[, 2])
) %>% 
  inner_join(
    IndividualIdentity_sel_tbl %>% 
      dplyr::select(sound.files, group_ID, individual_ID),
    by = c("sound.files")
  ) %>% 
  # glimpse()
  dplyr::mutate(
    unique_individuals = paste(group_ID, individual_ID, sep = " - "),
    group_ID = factor(group_ID),
    individual_ID = factor(individual_ID),
    unique_individuals = factor(unique_individuals)
  )

levels(mds_df2$group_ID)
levels(mds_df2$individual_ID)
levels(mds_df2$unique_individuals)

cols <- scales::alpha(c("navy", "orange"), 0.65)

shps <- c(0, 1, 2, 5, 6, 15, 19, 17, 18, 14)

hulls2 <- plyr::ddply(mds_df2, "group_ID", function(x){
  x[chull(x$X, x$Y), ]
})
```

Create an acoustic space plots for the vocalizations of this dataset.

```{r}

mds_df2 %>%
  ggplot(aes(x = X, y = Y, color = group_ID)) + 
  geom_polygon(data = hulls2, aes(x = X, y = Y, fill = group_ID, color = group_ID), alpha = 0.2, size = 0.2, show.legend = FALSE) +
  geom_point(aes(fill = group_ID, color = group_ID, shape = individual_ID), size = 2, stroke = 0.5) +
  scale_shape_manual(values = shps) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = cols) +
  xlab("MDS Dimension 1") + ylab("MDS Dimension 2") +
  guides(shape = guide_legend(nrow = 1, title = "Individual"), color = guide_legend(nrow = 1, title = "Group"), fill = guide_legend(nrow = 1, title = "Group")) +
  theme_bw() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 8),
    strip.text.x = element_text(size = 12),
    strip.text.y = element_text(size = 10),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.ticks = element_line(size = 0.25),
    plot.margin = margin(0, 3, -3, -3),
    legend.position = "top",
    legend.key.spacing = unit(0.1, "lines"),
    legend.box.spacing = unit(0.1, "lines")
  )

```

This will output an acoustic space plot to represent the acoustic similarity of the paRsynth generated vocalizations using the spectrograms created in vignette 02 for the second data set that has more individual information.

This last vignette has showed you how to conduct downstream bioacoustics analysis on two datasets: 1) more group information than individual information and 2) vice versa. You created acoustic space plots for each data sets. You have completed the final vignette of the paRsynth package, well done!:smile:

<h2>Extreme Group-level Variation Example</h2>

You may create vocalization that have much greater group information than individual information to the point when you make the acoustic space plot it result in all the individuals of one group to be clustered on top of each other and each group are a outstandingly great distance from each other. 

```{r eval = FALSE}

set.seed(123)
factor_ratio <- 10
individual_information <- 2
group_information <- individual_information * factor_ratio
n_groups <- 2
n_individuals <- 4
n_calls <- 3
string_length <- (group_information + individual_information) + 10

calls <- generate_strings(
  n_groups = n_groups, 
  n_individuals = n_individuals, 
  n_calls = n_calls, 
  string_length = string_length, 
  group_information = group_information, 
  individual_information = individual_information
  )

calls_parsons <- parsons_code(
  df = calls, 
  string_col = "Call", 
  mapping = list("A" = "up", "B" = "down", "C" = "constant")
)

calls_parsons_frequencies <- frequency_anchors(
  df = calls_parsons, 
  parsons_col = "Parsons_Code", 
  group_id_col = "Group", 
  individual_id_col = "Individual", 
  call_id_col = "Call_ID", 
  call_string_col = "Call", 
  starting_frequency = 4000, 
  frequency_shift = 1000
  )

synthetic_call_metadataEE <- write_audio(
  df = calls_parsons_frequencies, 
  save_path = audio_path, 
  sampling_rate = 150000, 
  sylLen = 400, 
  prefix = "ExtraExample"
  )

synthetic_call_metadataEE %>%
  write.csv(., file = file.path(analysis_path, "synthetic_call_metadataEE.csv"), row.names = FALSE) 

```

```{r}

synthetic_call_metadataEEcsv <- read.csv(file.path(analysis_path, "synthetic_call_metadataEE.csv"))

sel_tbl <- data.table::rbindlist(lapply(1:length(wavs), function(w){
  
  tmp <- tuneR::readWave(file.path(audio_path, wavs[w]))
  
  # Return the metadata for the given call using the synthetic metadata generated during audio file generation above
  
  synthetic_call_metadataEEcsv <- read.csv("/Users/gretheljuarez/Desktop/BIRDS/paRsynth_methods_synthetic_dataset/synthetic_call_metadataEE.csv")
  
  metadats_tmpEE <- synthetic_call_metadataEEcsv %>%
    dplyr::filter(grepl(wavs[w], audio_file_name))
  
  # glimpse(metadats_tmp)
  
  # Create a row for the selection table in warbleR format if the audio file exists and metadata for that file also exists
  if(nrow(metadats_tmpEE) > 0){
    
    # Use 0.1s as a margin to indicate where the vocalization starts and ends in the audio file
    # soundgen::soundgen() adds 100ms of silence before and after the synthetic vocalization by default 
    resEE <- data.frame(
      sound.files = wavs[w], 
      selec = 1,
      start = 0.1, 
      end = seewave::duration(tmp) - 0.1,
      sampling_rate = tmp@samp.rate,
      group_ID = metadats_tmpEE[["Group"]],
      individual_ID = metadats_tmpEE[["Individual"]],
      call_ID = metadats_tmpEE[["Call"]]
    )
    
  } else {
    
    resEE <- data.frame(
      sound.files = wavs[w], 
      selec = 1, 
      start = 0.1, 
      end = seewave::duration(tmp) + 0.1,
      sampling_rate = NA,
      group_ID = NA,
      individual_ID = NA,
      call_ID = NA
    )
    
  }
  
  return(resEE)
  
}))

ExtraExample_sel_tbl <- sel_tbl %>%
    dplyr::filter(grepl("ExtraExample", sound.files))

```

```{r eval = FALSE}

warbleR::spectrograms(ExtraExample_sel_tbl, wl = 512, flim = c(0, 12), wn = "hanning", pal = reverse.gray.colors.2,ovlp = 90, inner.mar = c(5, 4, 4, 2), outer.mar = c(0, 0, 0, 0), picsize = 1, res = 100, cexlab = 1, propwidth = FALSE, xl = 1, osci = FALSE, gr = FALSE, sc = FALSE, line = FALSE, mar = 0.05, it = "jpeg", parallel = 1, path = audio_path, pb = TRUE, fast.spec = FALSE, by.song = NULL, sel.labels = NULL, title.labels = NULL, dest.path = images_path, box = TRUE, axis = TRUE)

imgs <- list.files(images_path)

new_nms <- gsub(".wav-1", "", imgs)

invisible(file.rename(file.path(images_path, imgs), file.path(images_path, new_nms)))

```

```{r}

xc_matEE <- warbleR::cross_correlation(ExtraExample_sel_tbl, wl = 512, ovlp = 90, bp = c(0, 20), wn = "hanning", cor.method = "pearson", parallel = 1, na.rm = FALSE, type = "spectrogram", path = audio_path)

str(xc_matEE)
dim(xc_matEE)

xc_matEE[xc_matEE < 0]

neg_valuesEE <- length(xc_matEE[xc_matEE<0])/(dim(xc_matEE)[1]*dim(xc_matEE)[2])

if(neg_valuesEE > 0){
  stop("Negative values are present in the spectrographic cross-correlation matrix")
}

xc_matEE[xc_matEE < 0]

dimnames(xc_matEE) <- list(ExtraExample_sel_tbl$sound.files, ExtraExample_sel_tbl$sound.files)


dist_matEE <- stats::as.dist(1 - xc_matEE, diag = TRUE, upper = TRUE)

isoEE <- invisible(MASS::isoMDS(dist_matEE, k = 2, maxit = 1000, trace = FALSE))
str(isoEE)

mds_dfEE <- data.frame(
  sound.files = dimnames(xc_matEE)[[1]],
  X = as.vector(isoEE$points[, 1]), 
  Y = as.vector(isoEE$points[, 2])
) %>% 
  inner_join(
    ExtraExample_sel_tbl %>% 
      dplyr::select(sound.files, group_ID, individual_ID),
    by = c("sound.files")
  ) %>% 
  # glimpse()
  dplyr::mutate(
    unique_individuals = paste(group_ID, individual_ID, sep = " - "),
    group_ID = factor(group_ID),
    individual_ID = factor(individual_ID),
    unique_individuals = factor(unique_individuals)
  )

levels(mds_dfEE$group_ID)
levels(mds_dfEE$individual_ID)
levels(mds_dfEE$unique_individuals)

# Colors by group
cols <- scales::alpha(c("navy", "orange"), 0.65)

# Shapes by individual within groups
shps <- c(0, 1, 2, 5, 6, 15, 19, 17, 18, 14)

hullsEE <- plyr::ddply(mds_dfEE, "group_ID", function(x){
  x[chull(x$X, x$Y), ]
})

mds_dfEE %>%
  ggplot(aes(x = X, y = Y, color = group_ID)) +
  geom_polygon(data = hullsEE, aes(x = X, y = Y, fill = group_ID, color = group_ID), alpha = 0.2, size = 0.2, show.legend = FALSE) +
  geom_point(aes(fill = group_ID, color = group_ID, shape = individual_ID), size = 2, stroke = 0.5) +
  scale_shape_manual(values = shps) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = cols) +
  xlab("MDS Dimension 1") + ylab("MDS Dimension 2") +
  guides(shape = guide_legend(nrow = 1, title = "Individual"), color = guide_legend(nrow = 1, title = "Group"), fill = guide_legend(nrow = 1, title = "Group")) +
  theme_bw() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 8),
    strip.text.x = element_text(size = 12),
    strip.text.y = element_text(size = 10),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.ticks = element_line(size = 0.25),
    plot.margin = margin(0, 3, -3, -3),
    legend.position = "top",
    legend.key.spacing = unit(0.1, "lines"),
    legend.box.spacing = unit(0.1, "lines")
  )

```

This is an example of an acoustic plot of a dataset that has too much group information compared to individual information. Two unique dots can be seen at opposite ends of the plot - meaning there is not a lot of relevant information here to conduct acoustic analyses. To solve this, try changing the parameters so that the difference between them are not as striking.
